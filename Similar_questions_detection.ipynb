{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to get similar/duplicate questions\n",
    "\n",
    "1. Preprocess data - clean question title and body.\n",
    "remove html, punctuations, stopwords, words less than 4 chars, but need to keep words like SQL, C++, C#...\n",
    "\n",
    "2. Apply TF-IDF count vectorizer and create sparse matrix.\n",
    "\n",
    "3. Use NearestNeighbors and get most similar/duplicate 5 questions.\n",
    "\n",
    "Challenges - keeping words like SQL, C#, C++ ....\n",
    "\n",
    "For nearest neighbors, change metrics (mostly need to use cosine metric)\n",
    "\n",
    "How to relate title and body ??\n",
    "\n",
    "Can we use title , body and tag together to identify similar questions? Are tags reliable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from string import punctuation\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import ToktokTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsDF = pd.read_csv(\"Questions.csv\", encoding='ISO-8859-1')\n",
    "answersDF = pd.read_csv(\"Answers.csv\", encoding='ISO-8859-1')\n",
    "tagsDF = pd.read_csv(\"Tags.csv\", encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questionsDF = questionsDF.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = questionsDF['Body'].head(1).apply(lambda x: BeautifulSoup(x).get_text()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.value_counts(tagsDF['Tag'].head(1000)).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pd.Series(' '.join(tagsDF['Tag']).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "tagsDF['Tag'] = tagsDF['Tag'].astype(str)\n",
    "grouped_tags = tagsDF.groupby(\"Id\")['Tag'].apply(lambda tags: ' '.join(tags))\n",
    "grouped_tags.reset_index()\n",
    "grouped_tags_final = pd.DataFrame({'Id':grouped_tags.index, 'Tags':grouped_tags.values})\n",
    "#questionsDF.drop(columns=['OwnerUserId', 'CreationDate', 'ClosedDate'], inplace=True)\n",
    "df = questionsDF.merge(grouped_tags_final, on='Id')\n",
    "new_df = df[df['Score']>5]\n",
    "new_df.drop(columns=['Id', 'Score'], inplace=True)\n",
    "new_df['Tags'] = new_df['Tags'].apply(lambda x: x.split())\n",
    "\n",
    "\n",
    "# new_df = tagsDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in new_df['Tags'].values for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = questionsDF['Body']\n",
    "#test = questionsDF['Body'].head(10000).apply(lambda x: BeautifulSoup(x).get_text()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub(r\"\\'\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "test = test.apply(lambda x: clean_text(x)) \n",
    "token = ToktokTokenizer()\n",
    "punct = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "def strip_list_noempty(mylist):\n",
    "    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)\n",
    "    return [item for item in newlist if item != '']\n",
    "\n",
    "def clean_punct(text): \n",
    "    words=token.tokenize(text)\n",
    "    punctuation_filtered = []\n",
    "    regex = re.compile('[%s]' % re.escape(punct))\n",
    "    remove_punctuation = str.maketrans(' ', ' ', punct)\n",
    "    for w in words:\n",
    "        if w in flat_list:\n",
    "            punctuation_filtered.append(w)\n",
    "        else:\n",
    "            punctuation_filtered.append(regex.sub('', w))\n",
    "  \n",
    "    filtered_list = strip_list_noempty(punctuation_filtered)\n",
    "        \n",
    "    return ' '.join(map(str, filtered_list))\n",
    "\n",
    "test = test.apply(lambda x: clean_punct(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = test.str.join(' ')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train = vectorizer.fit_transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "model_knn = NearestNeighbors(n_neighbors=5,metric=\"cosine\").fit(x_train)\n",
    "distances, indices = model_knn.kneighbors(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0, 7678, 3782, 5444, 4659],\n",
       "        [   1,  748, 1682,  394,  255],\n",
       "        [   2, 1238, 2450,  201, 2005],\n",
       "        ...,\n",
       "        [9997, 7880,  173, 2041, 4309],\n",
       "        [9998, 9422, 8281, 3991, 8634],\n",
       "        [9999, 5234, 3649, 6205, 8961]]),\n",
       " array([[0.        , 0.76595854, 0.77973442, 0.79624491, 0.8076016 ],\n",
       "        [0.        , 0.78596133, 0.83338084, 0.84229343, 0.84646255],\n",
       "        [0.        , 0.71826958, 0.75315654, 0.76627653, 0.77252958],\n",
       "        ...,\n",
       "        [0.        , 0.78609394, 0.80441507, 0.81281784, 0.82508205],\n",
       "        [0.        , 0.60285395, 0.73795839, 0.78185969, 0.80519574],\n",
       "        [0.        , 0.71855159, 0.78059459, 0.78630231, 0.81248861]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am having a brain-dead moment i have two tables described by create table tablea id integer primary key autoincrement name varchar 255 not null unique name create table tableb id integer primary key autoincrement akey integer not null otherstuff varchar 255 not null foreign key akey references tablea id on delete cascade how can i select all rows from tablea that do not have an entry in tablebakey'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[7678]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i want to create a history table to track field changes across a number of tables in db2 i know history is usually done with copying an entire table structure and giving it a suffixed name eg user -- userhistory then you can use a pretty simple trigger to copy the old record into the history table on an update however for my application this would use too much space it does not seem like a good idea to me at least to copy an entire record to another table every time a field changes so i thought i could have a generic history table which would track individual field changes create table history historyid long generated always as identity recordid integer not null tablename varchar 32 not null fieldname varchar 64 not null fieldvalue varchar 1024 changetime timestamp primary key historyid ok so every table that i want to track has a single auto-generated id field as the primary key which would be put into the are cordid field and the maximum varchar size in the tables is 1024 obviously if a non-varchar field changes it would have to be converted into a varchar before inserting the record into the history table now this could be a completely retarded way to do things hey let me know why if it is but i think it it a good way of tracking changes that need to be pulled up rarely and need to be stored for a significant amount of time anyway i need help with writing the trigger to add records to the history table on an update let for example take a hypothetical user table create table user userid integer generated always as identity username varchar 32 not null firstname varchar 64 not null lastname varchar 64 not null emailaddress varchar 256 not null primary key userid so can anyone help me with a trigger on an update of the user table to insert the changes into the history table my guess is that some procedural sql will need to be used to loop through the fields in the old record compare them with the fields in the new record and if they do not match then add a new entry into the history table it would be preferable to use the same trigger action sql for every table regardless of its fields if it possible thanks'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[3782]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have built a database in ms access there i have a table called customers which also has a cell called employee type integer i also built a program in c++ which controls all data let say i have a string like this string sqlstring select from customers where customersemployee id id passes through my function correctly and is an integer so i get an error in compilation saying invalid pointer addition if i declare id as a string of course there no error but there are no results in my form also if i declare in database cell employee as text and build my query like this string sqlstring select from customers where customersemployee 128 i get results but i need that employee as an integer cause its a foreign key from another table so what should i do with my query to have results passing integer as parameter through variable id to be ok with the cell employee from database which is also integer any ideas i would really appreciate some help here as i said if i convert id to string there are no results in my form since employee in database is an integer so this std ostringstream buf buf select from customers where customersemployee id string str bufstr wo not do the job or any other conversion how can i pass id as an integer in my query'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[5444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is there a measurable performance difference between using int vs varchar as a primary key in mysql i would like to use varchar as the primary key for reference lists think us states country codes and a coworker wo not budge on the int autoincrement as a primary key for all tables my argument as detailed here is that the performance difference between int and varchar is negligible since every int foreign key reference will require a join to make sense of the reference a varchar key will directly present the information so does anyone have experience with this particular use-case and the performance concerns associated with it'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[4659]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = questionsDF[\"Body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Audio\n",
    "# sound_file = \"bottle_pop_2.wav\"\n",
    "# Audio(filename=sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_punctuation(words):\n",
    "#     \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    \n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "#         if new_word != '':\n",
    "#             new_words.append(new_word)\n",
    "#     return new_words\n",
    "\n",
    "\n",
    "\n",
    "# def lemmatize(words):\n",
    "#     text = \" \".join(WordNetLemmatizer().lemmatize(word) for word in words.split())\n",
    "#     return text.split()\n",
    "\n",
    "# def remove_less4(words):\n",
    "#     text = re.sub(r'\\b\\w{1,3}\\b', '', words)\n",
    "#     return text\n",
    "\n",
    "# def preprocess_data(text):  \n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'\\b\\w{1,3}\\b', '', text)   #remove words less than 3\n",
    "#     text = re.sub(r'[^\\w\\s]','',text)\n",
    "#     text = re.sub(r'\\d+', '', text)\n",
    "#     return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train = train.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "# train = train.str.replace('[^\\w\\s]','')\n",
    "# stop = stopwords.words('english')\n",
    "# train = train.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# freq = pd.Series(' '.join(train[1]).split()).value_counts()[:10]\n",
    "# freq = list(freq.index)\n",
    "# train[1] = train[1].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "# freq = pd.Series(' '.join(train[1]).split()).value_counts()[-10:]\n",
    "# freq = list(freq.index)\n",
    "# train[1] = train[1].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "# from textblob import TextBlob\n",
    "# TextBlob(train[1][1]).words\n",
    "# from textblob import Word\n",
    "# train[1] = train[1].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "\n",
    "# x_train = train[1]\n",
    "# #x_train = train[1].apply(preprocess_data)\n",
    "# #train[1] = train[1].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "# #freq = pd.Series(' '.join(x_train).split()).value_counts()[:30]\n",
    "# #x_train = x_train.apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "# # from IPython.display import Audio\n",
    "# # sound_file = \"bottle_pop_2.wav\"\n",
    "# # Audio(filename=sound_file, autoplay=True)\n",
    "# # x_train_unVectorized = x_train\n",
    "# #x_train = x_train.str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questionsDF.loc[questionsDF['Id'] == 80]\n",
    "#answersDF.loc[answersDF['ParentId'] == 80]\n",
    "import pickle\n",
    "filename = 'similar_questions_model'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(similar_questions_model,outfile)\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
